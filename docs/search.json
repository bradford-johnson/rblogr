[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog was originally going to be a book titled ‚ÄúR Cookbook‚Äù. While working on this book it became clearer that a blog would be a more effective medium to continuously update and add content to indefinitely.\n\n\nProgramming joke on converting my previous R Cookbook project to this blog made with Quarto named rblogr\n\nThe tutorials and content may vary as they will mostly contain concepts and solutions that relate to what I am currently working on. If you have any content recommendations or feedback, please reach out and message me!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "tables\n\n\ngt\n\n\n\n\n\n\n\n\n\n\n\nOct 10, 2022\n\n\nBradford Johnson\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nggplot2\n\n\n\n\n\n\n\n\n\n\n\nOct 4, 2022\n\n\nBradford Johnson\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSQL\n\n\nggplot2\n\n\n\n\n\n\n\n\n\n\n\nOct 3, 2022\n\n\nBradford Johnson\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nquarto\n\n\nSQL\n\n\nggplot2\n\n\n\n\n\n\n\n\n\n\n\nOct 1, 2022\n\n\nBradford Johnson\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nfunctions\n\n\noutliers\n\n\n\n\n\n\n\n\n\n\n\nSep 24, 2022\n\n\nBradford Johnson\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nrtweet\n\n\n\n\n\n\n\n\n\n\n\nSep 19, 2022\n\n\nBradford Johnson\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nweb scraping\n\n\nrvest\n\n\n\n\n\n\n\n\n\n\n\nSep 18, 2022\n\n\nBradford Johnson\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nSQL\n\n\n\n\n\n\n\n\n\n\n\nSep 17, 2022\n\n\nBradford Johnson\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ncleaning\n\n\nformattable\n\n\n\n\n\n\n\n\n\n\n\nSep 17, 2022\n\n\nBradford Johnson\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ncleaning\n\n\njanitor\n\n\n\n\n\n\n\n\n\n\n\nSep 16, 2022\n\n\nBradford Johnson\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ncleaning\n\n\noutliers\n\n\n\n\n\n\n\n\n\n\n\nSep 16, 2022\n\n\nBradford Johnson\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngithub\n\n\nprojects\n\n\nquarto\n\n\n\n\n\n\n\n\n\n\n\nSep 15, 2022\n\n\nBradford Johnson\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nquarto\n\n\n\n\n\n\n\n\n\n\n\nSep 15, 2022\n\n\nBradford Johnson\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/clean_names/index.html",
    "href": "posts/clean_names/index.html",
    "title": "Cleaning up Column Names with janitor",
    "section": "",
    "text": "Are they called column headers or column names? Perhaps it depends, but working with dirty ones can be avoided‚Ä¶\nWorking with dirty data is part of being a data analyst, and the janitor package is great because it can help clean up the column names so they are easier to work with. I will load in the readr package to import a hand crafted .csv that I made as an example. I will also load in the dplyr package so I can pipe the data into functions.\n\nClick on the hex janitor logo to see the documentation"
  },
  {
    "objectID": "posts/clean_names/index.html#the-packages",
    "href": "posts/clean_names/index.html#the-packages",
    "title": "Cleaning up Column Names with janitor",
    "section": "The Packages",
    "text": "The Packages\nHere is how you install and load the packages.\n\nInstall:\n\ninstall.packages(\"janitor\")\ninstall.packages(\"readr\")\ninstall.packages(\"dplyr\")\n\nLoad:\n\nlibrary(janitor)\nlibrary(readr)\nlibrary(dplyr)"
  },
  {
    "objectID": "posts/clean_names/index.html#importing-a-.csv-file",
    "href": "posts/clean_names/index.html#importing-a-.csv-file",
    "title": "Cleaning up Column Names with janitor",
    "section": "Importing a .csv File",
    "text": "Importing a .csv File\nImporting a .csv file is quite simple and using the readr package, the read_csv(\" \") function will let you import a .csv as a data frame and you can use <- or = to assign the data frame a name.\n\n# read in .csv\ndirty_df <- read_csv('janitor-example.csv')\n\n\nDirty Column Names\nHere is what the dirty column names on this data frame look like. The capitalization is all wild, there are spaces between characters, some of the names are not concise‚Ä¶\n\ndirty_df %>% \n  head()\n\n# A tibble: 6 x 3\n  `DAY OF THE WEEK` `TEMP F` `WEaThEr CONDITIONS`\n  <chr>                <dbl> <chr>               \n1 Monday                  98 sunny               \n2 Tuesday                 95 sunny               \n3 Wednesday               70 cloudy              \n4 Thursday                85 sunny               \n5 Friday                  83 sunny               \n6 Saturday                85 sunny"
  },
  {
    "objectID": "posts/clean_names/index.html#cleaning-column-names",
    "href": "posts/clean_names/index.html#cleaning-column-names",
    "title": "Cleaning up Column Names with janitor",
    "section": "Cleaning Column Names",
    "text": "Cleaning Column Names\nNow using the clean_names() function from the janitor package along with some mutate() functions I will load in the same data frame.\n\n# read in csv but with janitor and dplyr functions\nclean_df <- read_csv('janitor-example.csv') %>%\n  clean_names() %>%\n  mutate(weather_condition = w_ea_th_er_conditions) %>%\n  mutate(avg_temp_f = temp_f) %>%\n  mutate(weekday = day_of_the_week) %>%\n  select(weekday, avg_temp_f, weather_condition)\n\n\nCleaned Column Names\nHere is how the cleaned data frame looks. The column names are now easier to work with, and much better understood.\n\nclean_df %>%\n  head()\n\n# A tibble: 6 x 3\n  weekday   avg_temp_f weather_condition\n  <chr>          <dbl> <chr>            \n1 Monday            98 sunny            \n2 Tuesday           95 sunny            \n3 Wednesday         70 cloudy           \n4 Thursday          85 sunny            \n5 Friday            83 sunny            \n6 Saturday          85 sunny"
  },
  {
    "objectID": "posts/connect_database/index.html",
    "href": "posts/connect_database/index.html",
    "title": "Connect to a postgreSQL Database",
    "section": "",
    "text": "Connecting to a database can be an effective way to import data into R for analysis. In this post I will be showing how to connect to a postgreSQL database in RStudio."
  },
  {
    "objectID": "posts/connect_database/index.html#packages",
    "href": "posts/connect_database/index.html#packages",
    "title": "Connect to a postgreSQL Database",
    "section": "Packages",
    "text": "Packages\nYou will need the DBI, RPostgres, and dplyr packages.\ninstall.packages(\"DBI\")\ninstall.packages(\"RPostgres\")\ninstall.packages(\"dplyr\")\nOnce you install the packages you will need to load them.\nlibrary(DBI)\nlibrary(RPostgres)\nlibrary(dplyr)"
  },
  {
    "objectID": "posts/connect_database/index.html#connecting-to-the-database",
    "href": "posts/connect_database/index.html#connecting-to-the-database",
    "title": "Connect to a postgreSQL Database",
    "section": "Connecting to the Database",
    "text": "Connecting to the Database\nAfter loading the packages you can then connect to your database with this code and your database‚Äôs credentials. These credentials will be saved as con.\n# establish connection with postgres data base\ncon <- dbConnect(RPostgres::Postgres(),dbname = 'name',  \n      # enter name of database in the single quotes\n                 \n      host = 'address', \n# replace 'address' and enter the address of the database inside the single quotes\n                \n      port = 5432, \n# enter the port for the database \n      user = 'username', \n# replace 'username' with your username inside single quotes\n                 \n      password = 'password') \n# replace 'password' with your password inside single quotes"
  },
  {
    "objectID": "posts/connect_database/index.html#writing-a-sql-query-in-r",
    "href": "posts/connect_database/index.html#writing-a-sql-query-in-r",
    "title": "Connect to a postgreSQL Database",
    "section": "Writing a SQL Query in R",
    "text": "Writing a SQL Query in R\nAfter establishing your connection, you can then create your SQL query. It will be saved as res.\n# code to make sql query --inside of the double quotes you can create your query\nres <- dbSendQuery(con, \"\n                   SELECT COUNT(*)\n                   FROM ;\n                   \")\nTo execute your query you will then want to write and run this code, using dbFetch() with the name of your query object being the argument, in this case it is res. The results from this query will be saved as the name you assign it, for this code it is called df.\n# execute query\ndf <- dbFetch(res) \n# this will save the results of the query as an R data frame called 'DF'"
  },
  {
    "objectID": "posts/connect_database/index.html#disconnect-from-database",
    "href": "posts/connect_database/index.html#disconnect-from-database",
    "title": "Connect to a postgreSQL Database",
    "section": "Disconnect from Database",
    "text": "Disconnect from Database\nOnce you have run the query you can then clean the query and disconnect from the database using these functions and arguments respectively.\n# clear query\ndbClearResult(res)\n# disconnect from database\ndbDisconnect(con)"
  },
  {
    "objectID": "posts/functions/index.html",
    "href": "posts/functions/index.html",
    "title": "Functions in R",
    "section": "",
    "text": "Functions are quite important to conducting analysis, and they are not very hard to begin learning."
  },
  {
    "objectID": "posts/functions/index.html#r-script",
    "href": "posts/functions/index.html#r-script",
    "title": "Functions in R",
    "section": "R Script",
    "text": "R Script\nWhen learning how to write functions, I learned to write them from an already existing R script.\nFor example here is an existing block of code that I use for removing outliers from data.\nQ = quantile(outlier_df$var1.1, probs = c(.25,.75), na.rm = FALSE) \n\niqr = IQR(outlier_df$var1.1)  \n\nup <-  Q[2]+1.5*iqr # Upper Range  \nlow <- Q[1]-1.5*iqr # Lower Range  \n\nno_outliers_p <- outlier_df %>%  \n  filter(var1.1 > low & var1.1 < up)  \nThis is normal R code, and it is first getting the 1st and 3rd quartiles of a numeric variable. It then gets the IQR and subsequently gets the upper and lower ranges then filters the data, removing the outliers."
  },
  {
    "objectID": "posts/functions/index.html#r-function",
    "href": "posts/functions/index.html#r-function",
    "title": "Functions in R",
    "section": "R Function",
    "text": "R Function\nOne thing I have heard recently in my learning is that you should not repeat yourself. This means you should not be writing the same lines of code more than once. If you do, then write a function.\nI saw this as an opportunity to learn even more and challenge myself, so I decided to create my first function to help me with removing outliers.\nstrain <- function(x,column_q,column) {\n\n  Q <- quantile(x[[column_q]], probs = c(.25,.75), na.rm = FALSE)\n  iqr <- IQR(x[[column_q]])\n\n  up <- Q[2]+1.5*iqr\n  low <- Q[1]-1.5*iqr\n\n  strained_df <- x %>%\n    filter(column > low & column < up)\n\n  return(strained_df)\n}\nThe function and script look quite similar except in the function we are creating a more ‚Äúgeneric‚Äù script, that will run the same code the same way, for any data frame you input. This function is also going to be part of my own R Package, called strainerr (because it ‚Äústrains‚Äù out unwanted data)."
  },
  {
    "objectID": "posts/functions/index.html#arguments",
    "href": "posts/functions/index.html#arguments",
    "title": "Functions in R",
    "section": "Arguments",
    "text": "Arguments\nMy function calls for 3 arguments:\n\n\n\n\nArgument\n\n\n\n\n\nx\nA data frame\n\n\ncolumn_q\nThe column name in ‚Äúquotes‚Äù\n\n\ncolumn\nThe column name\n\n\n\n\nThe strain() function takes the arguments in this format:\nstrain(x, column_q, column)\nAn example with real arguments would look like this:\nstrain(data_df, \"ages\", ages)\nThis will remove outliers from the ages column in the data_df data frame."
  },
  {
    "objectID": "posts/github/index.html",
    "href": "posts/github/index.html",
    "title": "Why use GitHub with RStudio?",
    "section": "",
    "text": "Organizing projects in RStudio using GitHub is one thing I wish I had known about much sooner‚Ä¶\nAt first when I was learning R, I would only create R Script files and then save each plot as a .png. As time went on I discovered R Notebooks and R Markdown, these where perfect for practicing my R and having a single file with multiple different R Chunks that I could set up for each analysis stage. It was perfect because the file when saved would save a .html version, so it was a great way to work on and share projects.\nMore recently my projects became bigger and so did the files, but my local folder on my desktop named ‚ÄúData Science‚Äù became more filled. At the same time I was creating my first GitHub account so I uploaded some of these R projects into repositories. Once I would make changes locally I would then replace the respective file on GitHub. This is the extent I used GitHub to, as a ‚ÄúGoogle Drive‚Äù in a way.\nHowever I was looking through RStudio‚Äôs settings and noticed the Git/SVN option: \nAfter clicking the RStudio link I quickly learned you can connect RStudio to GitHub. I watched a few videos and I was able to connect the two together. I did find that having the desktop GitHub app helped with the initial set up and cloning your repositories.\nNow when making a project in R, I create a repository with it and can push / pull project changes live. I ended up creating a new GitHub account and starting fresh with this new organization. This is how I currently organize my GitHub:\n\n\nProjects get their own repositories.\n\nEach one gets their own ReadMe files that describes the project.\n\nSmaller files like individual R Scripts or R Notebooks go into my R repository, .SQL files go into the SQL repository and so on.\n\nEach main repository like R or SQL gets a ReadMe for describing my work in general and other details.\n\n\n\nIn addition to this, I keep a clone of each repository on my local machine as a backup, and as a way to edit and push/pull updates from GitHub.\nHaving a repository for each project is great as you can also have a GitHub Pages site for each of them to share your code, or even a deliverable. Using Quarto and GitHub Pages is such a smooth process to rapidly deploy reproducible deliverables for stakeholders. When setting up the Pages with GitHub it will automatically update the website with a push to the main branch, meaning I can update my website from RStudio."
  },
  {
    "objectID": "posts/github/index.html#using-git-in-rstudio",
    "href": "posts/github/index.html#using-git-in-rstudio",
    "title": "Why use GitHub with RStudio?",
    "section": "Using Git in RStudio",
    "text": "Using Git in RStudio\nIn RStudio you can access the Git tab once you are connected, and page will add records as your make changes to the files. You can check them off and click the green arrow to push the changes to GitHub.\n\nYou can also hit the Commit button to then see the below example:\n\n\nSteps to a Push\n\n\nCheck off all the changes on the left side.\nEnter a commit message.\nPress the commit button on the bottom right, a window like this should then pop up. \nClose out of the pop up window, then hit the green up arrow labeled ‚ÄúPush‚Äù and you have done a push.\n\nDone\n\n\n\n\nSteps to a Pull\n\n\nHit the blue down arrow labeled ‚ÄúPull‚Äù\n\nDone"
  },
  {
    "objectID": "posts/github/index.html#conclusion",
    "href": "posts/github/index.html#conclusion",
    "title": "Why use GitHub with RStudio?",
    "section": "Conclusion",
    "text": "Conclusion\nTo some this may seem like more to set up, and sure it may be a little more than just hitting save. The most important takeaway is that some type of file organization method needs to be implemented and sooner rather than later.\n\nThe benefits of using GitHub:\n\n\nHaving that central cloud based storage of files and projects.\nCollaborating with others can be easier with this method as everyone would need to have an account and be authorized to contribute to the repository, opposed to emailing or sending files back and forth.\nBranches for not losing your main files when making edits.\nAccess to GitHub Pages for projects (like this blog).\nBuild a portfolio.\nLearn how Git and GitHub works!"
  },
  {
    "objectID": "posts/gt/index.html",
    "href": "posts/gt/index.html",
    "title": "What can you do with gt?",
    "section": "",
    "text": "This code is from Kaustav Sen and to view their post click here.\n\n\nCode\nlibrary(tidyverse) # for reading in and wrangling the data\nlibrary(gt) # for awesome tables\n\nbeer_awards <- readr::read_csv(\"https://git.io/JTrx9\")\n\ntop_10_breweries <- \n  beer_awards %>% \n  count(brewery, medal) %>% \n  pivot_wider(names_from = medal, values_from = n) %>% \n  arrange(desc(Gold)) %>% \n  head(10)\n\ntop_10_breweries\n\n\n# A tibble: 10 x 4\n   brewery                           Gold Bronze Silver\n   <chr>                            <int>  <int>  <int>\n 1 Pabst Brewing Co.                   25     12     23\n 2 Firestone Walker Brewing Co.        20      6     13\n 3 Boston Beer Co.                     18      9      5\n 4 Miller Brewing Co.                  17     13     26\n 5 Anheuser-Busch, Inc                 16     18     20\n 6 New Belgium Brewing Co.             14      9      5\n 7 Alaskan Brewing and Bottling Co.    13      9     10\n 8 Marin Brewing Co.                   13      8      5\n 9 Pelican Pub & Brewery               12      6      6\n10 New Glarus Brewing Co.              11      8      7\n\n\n\n\n\nCode\ngt_table_step_1 <- gt(top_10_breweries)\ngt_table_step_1\n\n\n\n\n\n\n  \n  \n    \n      brewery\n      Gold\n      Bronze\n      Silver\n    \n  \n  \n    Pabst Brewing Co.\n25\n12\n23\n    Firestone Walker Brewing Co.\n20\n6\n13\n    Boston Beer Co.\n18\n9\n5\n    Miller Brewing Co.\n17\n13\n26\n    Anheuser-Busch, Inc\n16\n18\n20\n    New Belgium Brewing Co.\n14\n9\n5\n    Alaskan Brewing and Bottling Co.\n13\n9\n10\n    Marin Brewing Co.\n13\n8\n5\n    Pelican Pub & Brewery\n12\n6\n6\n    New Glarus Brewing Co.\n11\n8\n7\n  \n  \n  \n\n\n\n\n\n\n\nCode\ngt_table_step_2 <- gt_table_step_1 %>% \n  tab_style(\n    style = cell_text(color = \"#F2CB05\", weight = \"bold\"),\n    locations = cells_body(\n      columns = 1, \n      rows = brewery %in% c(\"Firestone Walker Brewing Co.\", \"Marin Brewing Co.\")\n    )\n  )\n\ngt_table_step_2\n\n\n\n\n\n\n  \n  \n    \n      brewery\n      Gold\n      Bronze\n      Silver\n    \n  \n  \n    Pabst Brewing Co.\n25\n12\n23\n    Firestone Walker Brewing Co.\n20\n6\n13\n    Boston Beer Co.\n18\n9\n5\n    Miller Brewing Co.\n17\n13\n26\n    Anheuser-Busch, Inc\n16\n18\n20\n    New Belgium Brewing Co.\n14\n9\n5\n    Alaskan Brewing and Bottling Co.\n13\n9\n10\n    Marin Brewing Co.\n13\n8\n5\n    Pelican Pub & Brewery\n12\n6\n6\n    New Glarus Brewing Co.\n11\n8\n7\n  \n  \n  \n\n\n\n\n\n\n\nCode\ngt_table_step_3 <- gt_table_step_2 %>% \n  tab_header(\n    title = \"Great American Beer Festival\",\n    subtitle = html(\n      \"<span style = 'color: grey'>All time top 10 breweries of which 2 \n      are <span style = 'color: #F2CB05'><b>California</b></span> based</span>\"\n    )\n  ) %>% \n  tab_style(\n    style = cell_text(\n      font = google_font(\"Titan One\"), \n      align = \"left\", \n      size = \"xx-large\"\n    ),\n    locations = cells_title(\"title\")\n  ) %>% \n  tab_style(\n    style = cell_text(\n      font = google_font(\"IBM Plex Sans\"), \n      align = \"left\", \n      size = \"large\"\n    ),\n    locations = cells_title(\"subtitle\")\n  ) %>% \n  tab_style(\n    style = cell_borders(\n      sides = \"bottom\", \n      color = \"#ebe8e8\", \n      weight = px(2)\n    ),\n    locations = cells_title(\"subtitle\")\n  ) %>%\n  tab_spanner(\n    label = \"Medals Won\",\n    columns = 2:4\n  ) %>% \n  tab_style(\n    style = cell_text(\n      font = google_font(\"IBM Plex Sans\"), \n      size = \"large\"\n    ),\n    locations = list(\n      cells_column_labels(everything()), \n      cells_body(columns = 1)\n    )\n  ) %>%  \n  tab_style(\n    style = cell_text(\n      font = google_font(\"IBM Plex Sans\"), \n      size = \"medium\", \n      weight = \"bold\"\n    ),\n    locations = cells_column_spanners(\"Medals Won\")\n  ) %>% \n  tab_style(\n    style = cell_text(\n      font = google_font(\"IBM Plex Mono\"), \n      size = \"large\"\n    ),\n    locations = cells_body(columns = 2:4)\n  )\n\ngt_table_step_3\n\n\n\n\n\n\n  \n    \n      Great American Beer Festival\n    \n    \n      All time top 10 breweries of which 2 \n      are California based\n    \n  \n  \n    \n      brewery\n      \n        Medals Won\n      \n    \n    \n      Gold\n      Bronze\n      Silver\n    \n  \n  \n    Pabst Brewing Co.\n25\n12\n23\n    Firestone Walker Brewing Co.\n20\n6\n13\n    Boston Beer Co.\n18\n9\n5\n    Miller Brewing Co.\n17\n13\n26\n    Anheuser-Busch, Inc\n16\n18\n20\n    New Belgium Brewing Co.\n14\n9\n5\n    Alaskan Brewing and Bottling Co.\n13\n9\n10\n    Marin Brewing Co.\n13\n8\n5\n    Pelican Pub & Brewery\n12\n6\n6\n    New Glarus Brewing Co.\n11\n8\n7\n  \n  \n  \n\n\n\n\n\n\n\nCode\nlibrary(emo)\ngt_table_step_4 <- gt_table_step_3 %>% \n  cols_label(\n    brewery = \"\",\n    Gold = emo::medal(1),\n    Silver = emo::medal(3),\n    Bronze = emo::medal(2)\n  )\n\ngt_table_step_4\n\n\n\n\n\n\n  \n    \n      Great American Beer Festival\n    \n    \n      All time top 10 breweries of which 2 \n      are California based\n    \n  \n  \n    \n      \n      \n        Medals Won\n      \n    \n    \n      ü•á\n      ü•à\n      ü•â\n    \n  \n  \n    Pabst Brewing Co.\n25\n12\n23\n    Firestone Walker Brewing Co.\n20\n6\n13\n    Boston Beer Co.\n18\n9\n5\n    Miller Brewing Co.\n17\n13\n26\n    Anheuser-Busch, Inc\n16\n18\n20\n    New Belgium Brewing Co.\n14\n9\n5\n    Alaskan Brewing and Bottling Co.\n13\n9\n10\n    Marin Brewing Co.\n13\n8\n5\n    Pelican Pub & Brewery\n12\n6\n6\n    New Glarus Brewing Co.\n11\n8\n7\n  \n  \n  \n\n\n\n\n\n\n\nCode\nplot_barchart <- function(brewery, data) {\n  \n  plot_data <- \n    beer_awards %>% \n    filter(brewery == {{ brewery }}) %>% \n    count(year)\n\n  plot <- \n    plot_data %>% \n    ggplot(aes(year, n)) +\n    geom_col(fill = \"#F28705\", alpha = 0.75) +\n    geom_segment(aes(x = 1986.5, xend = 1986.5, y = -0.1, yend = -0.5)) +\n    geom_segment(aes(x = 2020.5, xend = 2020.5, y = -0.1, yend = -0.5)) +\n    geom_segment(aes(x = 1986.5, xend = 2020.5, y = -0.1, yend = -0.1)) +\n    annotate(\"text\", x = 1986.5, y = -1.25, \n             label = \"1987\", size = 10, color = \"grey40\") +\n    annotate(\"text\", x = 2020.5, y = -1.25, \n             label = \"2020\", size = 10, color = \"grey40\") +\n    scale_x_continuous(limits = c(1970, 2035)) +\n    scale_y_continuous(limits = c(-4, 10)) +\n    theme_void()    \n    \n  plot\n  \n}\n\n\n\n\n\nCode\n# devtools::install_github(\"hadley/emo\", force = TRUE)\nlibrary(emo)\n\n\n\n\n\nCode\ntop_10_breweries_with_graphs <- top_10_breweries %>% \n  mutate(plots = purrr::map(brewery, plot_barchart, data = beer_awards))\n\n\n\n\n\nCode\ngt_table_step_5 <- gt(top_10_breweries_with_graphs) %>% \n  tab_style(\n    style = cell_text(color = \"#F2CB05\", weight = \"bold\"),\n    locations = cells_body(\n      columns = 1, \n      rows = brewery %in% c(\"Firestone Walker Brewing Co.\", \"Marin Brewing Co.\")\n    )\n  ) %>% \n  tab_header(\n    title = \"Great American Beer Festival\",\n    subtitle = html(\n      \"<span style = 'color: grey'>All time top 10 breweries of which 2 \n      are <span style = 'color: #F2CB05'><b>California</b></span> based</span>\"\n    )\n  ) %>% \n  tab_style(\n    style = cell_text(\n      font = google_font(\"Titan One\"), \n      align = \"left\", \n      size = \"xx-large\"\n    ),\n    locations = cells_title(\"title\")\n  ) %>% \n  tab_style(\n    style = cell_text(\n      font = google_font(\"IBM Plex Sans\"), \n      align = \"left\", \n      size = \"large\"\n    ),\n    locations = cells_title(\"subtitle\")\n  ) %>% \n  tab_style(\n    style = cell_borders(\n      sides = \"bottom\", \n      color = \"#ebe8e8\", \n      weight = px(2)\n    ),\n    locations = cells_title(\"subtitle\")\n  ) %>%\n  tab_spanner(\n    label = \"Medals Won\",\n    columns = 2:4\n  ) %>% \n  tab_style(\n    style = cell_text(\n      font = google_font(\"IBM Plex Sans\"), \n      size = \"large\"\n    ),\n    locations = list(\n      cells_column_labels(everything()), \n      cells_body(columns = 1)\n    )\n  ) %>%  \n  tab_style(\n    style = cell_text(\n      font = google_font(\"IBM Plex Sans\"), \n      size = \"medium\", \n      weight = \"bold\"\n    ),\n    locations = cells_column_spanners(\"Medals Won\")\n  ) %>% \n  tab_style(\n    style = cell_text(\n      font = google_font(\"IBM Plex Mono\"), \n      size = \"large\"\n    ),\n    locations = cells_body(columns = 2:4)\n  ) %>% \n  cols_label(\n    brewery = \"\",\n    Gold = emo::medal(1),\n    Silver = emo::medal(3),\n    Bronze = emo::medal(2)\n  ) %>% \n  #------------------------------------#\n  #  This is the section which is new  #\n  #------------------------------------#\n  text_transform(\n    locations = cells_body(vars(plots)),\n    fn = function(x) {\n      map(top_10_breweries_with_graphs$plots, ggplot_image, height = px(120), aspect_ratio = 1.5)\n    }\n  )\n\ngt_table_step_5\n\n\n\n\n\n\n  \n    \n      Great American Beer Festival\n    \n    \n      All time top 10 breweries of which 2 \n      are California based\n    \n  \n  \n    \n      \n      \n        Medals Won\n      \n      plots\n    \n    \n      ü•á\n      ü•à\n      ü•â\n    \n  \n  \n    Pabst Brewing Co.\n25\n12\n23\n\n    Firestone Walker Brewing Co.\n20\n6\n13\n\n    Boston Beer Co.\n18\n9\n5\n\n    Miller Brewing Co.\n17\n13\n26\n\n    Anheuser-Busch, Inc\n16\n18\n20\n\n    New Belgium Brewing Co.\n14\n9\n5\n\n    Alaskan Brewing and Bottling Co.\n13\n9\n10\n\n    Marin Brewing Co.\n13\n8\n5\n\n    Pelican Pub & Brewery\n12\n6\n6\n\n    New Glarus Brewing Co.\n11\n8\n7\n\n  \n  \n  \n\n\n\n\n\n\n\nCode\ngt_table_step_final <- gt_table_step_5 %>% \n  cols_label(\n    plots = md(\"**Medal Distribution<br>1987-2020**\")\n  ) %>% \n  tab_source_note(md(\"**Data**: Great American Beer Festival | **Table**: Kaustav Sen\")) %>% \n  cols_width(\n    1 ~ px(300),\n    2:4 ~ px(50)\n  ) %>%\n  opt_table_font(font = google_font(\"IBM Plex Sans\")) %>%  # Used to set the font for the source note\n  tab_options(\n    column_labels.border.top.color = \"white\",\n    column_labels.border.bottom.color = \"black\",\n    table.border.top.color = \"white\",\n    table.border.bottom.color = \"white\",\n    table_body.hlines.color = \"white\"\n  )\ngt_table_step_final\n\n\n\n\n\n\n  \n    \n    \n    \n    \n    \n  \n  \n    \n      Great American Beer Festival\n    \n    \n      All time top 10 breweries of which 2 \n      are California based\n    \n  \n  \n    \n      \n      \n        Medals Won\n      \n      Medal Distribution1987-2020\n    \n    \n      ü•á\n      ü•à\n      ü•â\n    \n  \n  \n    Pabst Brewing Co.\n25\n12\n23\n\n    Firestone Walker Brewing Co.\n20\n6\n13\n\n    Boston Beer Co.\n18\n9\n5\n\n    Miller Brewing Co.\n17\n13\n26\n\n    Anheuser-Busch, Inc\n16\n18\n20\n\n    New Belgium Brewing Co.\n14\n9\n5\n\n    Alaskan Brewing and Bottling Co.\n13\n9\n10\n\n    Marin Brewing Co.\n13\n8\n5\n\n    Pelican Pub & Brewery\n12\n6\n6\n\n    New Glarus Brewing Co.\n11\n8\n7\n\n  \n  \n    \n      Data: Great American Beer Festival | Table: Kaustav Sen"
  },
  {
    "objectID": "posts/outliers/index.html",
    "href": "posts/outliers/index.html",
    "title": "Identifying and Removing Outliers",
    "section": "",
    "text": "Outliers can distort a dataset, but how do you find and remove them?"
  },
  {
    "objectID": "posts/outliers/index.html#what-are-outliers",
    "href": "posts/outliers/index.html#what-are-outliers",
    "title": "Identifying and Removing Outliers",
    "section": "What are outliers?",
    "text": "What are outliers?\nThis image is from DataCamp‚Äôs learning platform, and it shows a visual of a boxplot, and the outliers on either side of the distribution. To find the outliers on the lower range you do the following equation Q1 - 1.5 x IQR. To find outliers on the upper range you use this equation Q3 + 1.5 * IQR.\n\nYou can use those equations to then filter out the outliers and then continue with your analysis. I have created some data with outliers to show how you would do this in R."
  },
  {
    "objectID": "posts/outliers/index.html#steps-to-identify-outliers",
    "href": "posts/outliers/index.html#steps-to-identify-outliers",
    "title": "Identifying and Removing Outliers",
    "section": "Steps to identify outliers:",
    "text": "Steps to identify outliers:\n\nGet the 25th and 75th percentile of a dataset\n\n    Q = quantile(DATAFRAME$columnName, probs = c(.25,.75), na.rm = FALSE)\n\nFind the Interquartile Range (IQR)\n\n    iqr = IQR(DATAFRAMEf$columnName)\n\nFind upper and lower cut off ranges\n\n    up <-  Q[2]+1.5*iqr # Upper Range\n    low <- Q[1]-1.5*iqr # Lower Range"
  },
  {
    "objectID": "posts/outliers/index.html#steps-to-remove-outliers",
    "href": "posts/outliers/index.html#steps-to-remove-outliers",
    "title": "Identifying and Removing Outliers",
    "section": "Steps to remove outliers:",
    "text": "Steps to remove outliers:\n\nIdentify outliers\n\nAs shown above\n\nUse dplyr to filter\ninstall.packages(\"dplyr\") \nlibrary(dplyr)\nFilter\nclean_dataframe <- DATAFRAME %>%\n  filter(columnName > low & columnName < up)"
  },
  {
    "objectID": "posts/outliers/index.html#here-is-an-example-of-the-full-code-working-with-a-data-frame",
    "href": "posts/outliers/index.html#here-is-an-example-of-the-full-code-working-with-a-data-frame",
    "title": "Identifying and Removing Outliers",
    "section": "Here is an example of the full code working with a data frame",
    "text": "Here is an example of the full code working with a data frame\n\n# remove outliers steps\n# 1. get Q1 and Q3 \nQ = quantile(outlier_df$var1.1, probs = c(.25,.75), na.rm = FALSE)\n# 2. get IQR\niqr = IQR(outlier_df$var1.1)\n# 3. get upper and lower ranges\nup <-  Q[2]+1.5*iqr # Upper Range  \nlow <- Q[1]-1.5*iqr # Lower Range\n# 4. remove outliers (outlier_df is the name of my data frame, var1.1 is the name of the column that I am removing outliers from)\nno_outliers_p <- outlier_df %>%\n  filter(var1.1 > low & var1.1 < up) %>%\n  ggplot(aes(x = var1.1)) +\n  geom_boxplot() + \n  theme_classic() +\n  labs(title = \"Without Outliers\") +\n  theme(axis.text.y = element_blank(), axis.ticks.y = element_blank())"
  },
  {
    "objectID": "posts/outliers/index.html#with-outliers-vs-without-outliers",
    "href": "posts/outliers/index.html#with-outliers-vs-without-outliers",
    "title": "Identifying and Removing Outliers",
    "section": "With Outliers vs Without Outliers",
    "text": "With Outliers vs Without Outliers\n\n# patchwork to show plots\noutlier_p / no_outliers_p"
  },
  {
    "objectID": "posts/percents/index.html",
    "href": "posts/percents/index.html",
    "title": "Adding Percent Signs with formattable",
    "section": "",
    "text": "Have you wanted to create a visual or table, and struggled to figure out how to change decimals to percents in R?\n\n\nHere is how you install and load the package, and you can click here to see the documentation.\ninstall.packages(\"formattable\")\nlibrary(formattable)\nI will also be loading the tidyverse and janitor packages in the background.\n\n\n\nHere is how you would break down the iris dataset by species, using the formattable package to get percents of each species.\n\niris_df %>%\n  group_by(species) %>%\n  summarise(cnt = n()) %>%\n  mutate(freq = formattable::percent(cnt / sum(cnt)))\n\n# A tibble: 3 x 3\n  species      cnt freq      \n  <fct>      <int> <formttbl>\n1 setosa        50 33.33%    \n2 versicolor    50 33.33%    \n3 virginica     50 33.33%"
  },
  {
    "objectID": "posts/radial/index.html",
    "href": "posts/radial/index.html",
    "title": "Wind Direction Radial",
    "section": "",
    "text": "Wind Bearing\n\n\nCode\nlibrary(DBI)\nlibrary(RPostgres)\nlibrary(tidyverse)\nlibrary(strainerr)\n\n\n\n\nCode\n# make sql query\nres <- dbSendQuery(con, \"\n              SELECT windbearing AS angle, COUNT(windbearing) AS frequency \n              FROM weatherinszeged\n              GROUP BY windbearing\n              ORDER BY angle;\n                   \")\n# execute query\ndf <- dbFetch(res)\n\n# clear query\ndbClearResult(res)\n\n# disconnect from data base\ndbDisconnect(con)\n\np <- df %>%\n  mutate(Frequency = as.numeric(frequency), angle = as.numeric(angle)) %>%\n  ggplot(aes(x = angle, y = Frequency,colour = Frequency)) +\n  coord_polar(theta = \"x\", start = -pi/45) +\n  geom_col(position=\"jitter\",size = 1) +\n  scale_x_continuous(breaks = seq(0, 360, 45)) +\n  scale_colour_continuous(type = \"gradient\") +\n  theme_minimal() +\n  theme(legend.position = \"right\", legend.title = element_text(angle = 0, vjust = 1), axis.text.y = element_blank(), axis.text.x = element_text(face = \"bold\"))\n\np <- titlr(p, \"\", \"Angle\", \"\")\n\np\n\n\n\n\n\n\n\nWind Bearing and Speed\n\n\nCode\n# make sql query\nres <- dbSendQuery(con, \"\n              SELECT windspeed AS wind, windbearing as bear\n              FROM weatherinszeged\n              LIMIT 2000;\n                   \")\n# execute query\ndf2 <- dbFetch(res)\n\n# clear query\ndbClearResult(res)\n\n# disconnect from data base\ndbDisconnect(con)\n\np2 <- df2 %>%\n  mutate(MPH=wind) %>%\nggplot(aes(x = bear, y = MPH,colour = MPH)) +\n  coord_polar(theta = \"x\", start = -pi/45) +\n  geom_col(position = \"jitter\",size = 1) +\n  scale_x_continuous(breaks = seq(0, 360, 45)) +\n  scale_colour_continuous(type = \"gradient\") +\n  theme_minimal() +\n  theme(legend.position = \"right\", legend.title = element_text(angle = 0, vjust = 1), axis.text.y = element_blank(), axis.text.x = element_text(face = \"bold\"))\n  \np2 <- titlr(p2,\"\", \"Angle\", \"\")\n\np2"
  },
  {
    "objectID": "posts/rtweet/index.html",
    "href": "posts/rtweet/index.html",
    "title": "Tweet Scraping with rtweet",
    "section": "",
    "text": "Tons of data is created on social media, and using social media can be a great source for businesses to learn more about their customers.\nYou can access data from Twitter by making a Twitter Developer‚Äôs Account and by using the Twitter API. After making a developers account it is very easy to get the data using the rtweet package.\nClick on the hex rtweet logo to access the documentation.\n\n\n\n\n\nFirst you need to make sure you default browser is open and you are logged into your Twitter account. After you do this make sure to install the rtweet package and load it in RStudio.\n\ninstall.packages(\"rtweet\")\nlibrary(rtweet)\n\nAfter installing the package and loading it, you can use this code to set up your authentication and the search_tweets() function can retrieve Tweet data based on your parameters.\nIn this call of search_tweets() it will search for Tweets that contain ‚Äúrocket league‚Äù, n = 100 means it will return 100 tweets, include_rts = FALSE means that it will not include and return Retweets, and finally lang = \"en\" means it will only return Tweets in English.\n\nauth_setup_default()\nauth_has_default()\nrt <- search_tweets(\"rocket league\", n = 100, include_rts = FALSE, lang = \"en\")"
  },
  {
    "objectID": "posts/sql-ggplot/index.html",
    "href": "posts/sql-ggplot/index.html",
    "title": "Airline Tweet Sentiment Database",
    "section": "",
    "text": "Code\nlibrary(DBI)\nlibrary(RPostgres)\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(strainerr)\n\n\n\n\n\n\n\nCode\nres <- dbSendQuery(con, \"\n\n  SELECT airline, COUNT(negativereason) as n_tweets\n  FROM codedtweets\n  GROUP BY airline\n  ORDER BY n_tweets DESC;\n                   \n                   \")\n\nairline_df <- dbFetch(res)\ndbClearResult(res)\n\n\n\n\nCode\ncolors_df <-c(\"#003f5c\",\"#58508d\",\"#bc5090\",\"#ff6361\",\"#ffa600\")\np1 <- airline_df %>%\n  mutate(n_tweets = as.numeric(n_tweets), \n         airline = factor(airline, levels = c(\"United\",\n                                              \"US Airways\",\n                                              \"Southwest\",\n                                              \"Delta\",\n                                              \"Virgin America\"))) %>%\n  ggplot(aes(x=airline, y=n_tweets, fill=colors_df, label=n_tweets)) +\n  geom_col() +\n  geom_label(colour=\"white\",nudge_y = 120, alpha = .7)\n  \np1 <- titlr(p1, \"Negative Tweets by Airline\", \"\", \"Count of Tweets\")\n\np1 <- p1 + scale_fill_manual(values = colors_df) + theme_classic()\n\np1 <- p1 + theme(legend.position = \"\", axis.text.x = element_text(face = \"bold\"))\n\np1\n\n\n\n\n\nFigure 1: Breakdown of negative Tweets by airline\n\n\n\n\n\n\nCode\nres <- dbSendQuery(con, \"\n\n  -- Breakdown by airline and reason\n  SELECT airline, negativereason, COUNT(negativereason) as n_tweets\n  FROM codedtweets\n  GROUP BY airline, negativereason\n  HAVING(COUNT(negativereason)) > 0\n  ORDER BY airline ASC, n_tweets DESC;\n                   \n                   \")\n\nairline_reason_df <- dbFetch(res)\ndbClearResult(res)\n\n\n\n\nCode\np2 <- airline_reason_df %>%\n  mutate(n_tweets = as.numeric(n_tweets), \n         airline = factor(airline, levels = c(\"United\",\n                                              \"US Airways\",\n                                              \"Southwest\",\n                                              \"Delta\",\n                                              \"Virgin America\")),\n         negativereason = case_when(negativereason == \"Customer Service Issue\" ~ \"CX\",\n                                    negativereason == \"Can\\'t Tell\" ~ \"Unknown\",\n                                    negativereason == \"Bad Flight\" ~ \"FX\",\n                                    negativereason == \"Flight Attendant Complaints\" ~ \"FX\",\n                                    negativereason == \"Cancelled Flight\" ~ \"FX\", \n                                    negativereason == \"Flight Booking Problems\" ~ \"CX\",\n                                    negativereason == \"longlines\" ~ \"CX\",\n                                    negativereason == \"Late Flight\" ~ \"FX\",\n                                    negativereason == \"Lost Luggage\" ~ \"Luggage Issue\",\n                                    negativereason == \"Damaged Luggage\" ~ \"Luggage Issue\")) %>%\n  ggplot(aes(x=negativereason, y=n_tweets, fill=airline, label=n_tweets)) +\n  geom_col(position = \"dodge\") \n  \np2 <- titlr(p2, \"Negative Tweet Topic by Airline\", \"\", \"Count of Tweets\")\n\np2 <- p2 + scale_fill_manual(values=colors_df) + theme_classic()\n\np2 <- p2 + theme(legend.position = \"right\", legend.box.spacing = unit(.4,\"cm\") ,legend.title = element_blank(),legend.key.size = unit(.4,\"cm\"), axis.text.x = element_text(angle = 0, vjust = .4, face = \"bold\"))\n\np2 \n\n\n\n\n\nFigure 2: Breakdown of negative Tweet topic by Airline\n\n\n\n\n\nCX = Customer Experience\nFX = Flight Experience"
  },
  {
    "objectID": "posts/timeline/index.html",
    "href": "posts/timeline/index.html",
    "title": "Curriculum Vitae Timeline with ggplot2",
    "section": "",
    "text": "You can use this code to create your own timeline!\n\nThis is what mine looks like‚Ä¶\nBig thanks to Antoine Soetewey for sharing this code in this post.\n\n\n\nCode\n# All packages used below must be installed first\n#library(devtools)\n#devtools::install_github(\"laresbernardo/lares\")\n#library(lares)\nlibrary(ggplot2)\n\n\ntoday <- as.character(Sys.Date())\n\n\n### Edit from here ###\ncv <- data.frame(rbind(\n  c(\"BA Sociology\", \"Winthrop University\", \"Academic\", \"2017-09-01\", \"2021-12-18\"),\n  c(\"Data Analytics Program\", \"Thinkful\", \"Academic\", \"2022-06-01\", today),\n  c(\"IT Team Lead\", \"TopGolf\", \"Work Experience\", \"2022-10-03\", today),\n  c(\"Maintenance Technician\", \"TopGolf\", \"Work Experience\", \"2022-03-01\", \"2022-10-2\"),\n  c(\"Time Using R\", \"R\", \"Extra\", \"2021-01-01\", today)\n))\n### Edit until here ###\n\n\norder <- c(\"Role\", \"Place\", \"Type\", \"Start\", \"End\")\ncolnames(cv) <- order\n\n\nplot_timeline2 <- function(event, start, end = start + 1, label = NA, group = NA,\n                           title = \"Curriculum Vitae Timeline\", subtitle = \"Bradford Johnson\",\n                           size = 7, colour = \"orange\", save = FALSE, subdir = NA) {\n  df <- data.frame(\n    Role = as.character(event), Place = as.character(label),\n    Start = lubridate::date(start), End = lubridate::date(end),\n    Type = group\n  )\n  cvlong <- data.frame(pos = rep(\n    as.numeric(rownames(df)),\n    2\n  ), name = rep(as.character(df$Role), 2), type = rep(factor(df$Type,\n                                                             ordered = TRUE\n  ), 2), where = rep(\n    as.character(df$Place),\n    2\n  ), value = c(df$Start, df$End), label_pos = rep(df$Start +\n                                                    floor((df$End - df$Start) / 2), 2))\n  maxdate <- max(df$End)\n  p <- ggplot(cvlong, aes(\n    x = value, y = reorder(name, -pos),\n    label = where, group = pos\n  )) +\n    geom_vline(\n      xintercept = maxdate,\n      alpha = 0.8, linetype = \"dotted\"\n    ) +\n    labs(\n      title = title,\n      subtitle = subtitle, x = NULL, y = NULL, colour = NULL\n    ) +\n    theme_classic() +\n    theme(panel.background = element_rect(\n      fill = \"white\",\n      colour = NA\n    ), axis.ticks = element_blank(), panel.grid.major.x = element_line(\n      size = 0.25,\n      colour = \"grey80\"\n    ))\n  if (!is.na(cvlong$type)[1] | length(unique(cvlong$type)) >\n      1) {\n    p <- p + geom_line(aes(color = type), size = size) +\n      facet_grid(type ~ ., scales = \"free\", space = \"free\") +\n      guides(colour = \"none\") +\n      scale_colour_manual(values = c(\"#F8766D\", \"#00BA38\", \"#619CFF\")) +\n      theme(strip.text.y = element_text(size = 10))\n  } else {\n    p <- p + geom_line(size = size)\n  }\n  p <- p + geom_label(aes(x = label_pos),\n                      colour = \"black\",\n                      size = 2, alpha = 0.7\n  )\n  if (save) {\n    file_name <- \"cv_timeline.png\"\n    if (!is.na(subdir)) {\n      dir.create(file.path(getwd(), subdir), recursive = T)\n      file_name <- paste(subdir, file_name, sep = \"/\")\n    }\n    p <- p + ggsave(file_name, width = 8, height = 6)\n    message(paste(\"Saved plot as\", file_name))\n  }\n  return(p)\n}\n\n\n\n\nplot_timeline2(\n  event = cv$Role,\n  start = cv$Start,\n  end = cv$End,\n  label = cv$Place,\n  group = cv$Type,\n  save = FALSE,\n  subtitle = \"Bradford Johnson\" # replace with your name\n)"
  },
  {
    "objectID": "posts/webscrape/index.html",
    "href": "posts/webscrape/index.html",
    "title": "Web Scraping with rvest",
    "section": "",
    "text": "Using the rvest package you can get the data you need from webpages and quickly use it for analysis.\nBelow I will show a simple script using the rvest, lubridate and tidyverse packages that can scrape us some data from Steam‚Äôs game stats page. Steam is a video game distribution service, and we will scrape a couple columns from their live Top games by current player count table.\nFirst of course install and load the packages, if you already have any of these packages then you just need to load them."
  },
  {
    "objectID": "posts/webscrape/index.html#install-and-load-packages",
    "href": "posts/webscrape/index.html#install-and-load-packages",
    "title": "Web Scraping with rvest",
    "section": "Install and Load Packages",
    "text": "Install and Load Packages\ninstall.packages(\"tidyverse\")\ninstall.packages(\"rvest\")\ninstall.packages(\"lubridate\")\n\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(lubridate)\nIf you want to see the documentation for any of these packages then click their hex image."
  },
  {
    "objectID": "posts/webscrape/index.html#setting-the-parameters",
    "href": "posts/webscrape/index.html#setting-the-parameters",
    "title": "Web Scraping with rvest",
    "section": "Setting the Parameters",
    "text": "Setting the Parameters\nNext you will set up the parameters so rvest knows where to get the data from. The html_nodes can be found using the browser extension SelectorGadget found here. Using this extension you can highlight what you want to web scrape and copy/paste the html nodes from SelectorGadget.\n# link to get data from\nlink = \"https://store.steampowered.com/stats/\" \n# read webpage at the above link\npage = read_html(link) \n# scrape top 100 games by current players\ngame = page %>% \n  html_nodes(\".gameLink\") %>% \n  html_text()  \n# scrape number of players for each game \ncurrent_players = page %>% \n  html_nodes(\"td:nth-child(1) .currentServers\") %>% \n  html_text()"
  },
  {
    "objectID": "posts/webscrape/index.html#creating-the-data-frame",
    "href": "posts/webscrape/index.html#creating-the-data-frame",
    "title": "Web Scraping with rvest",
    "section": "Creating the Data Frame",
    "text": "Creating the Data Frame\nAfter getting all the data you will want to put it into a data frame to work with it, so you will then use the data.frame() function and add in the data you pulled from online. Below you will see how I am creating the data frame and adding in the current date as a column so I know when I collected this data.\n# put both game and player data into a data frame\ndf = data.frame(game, current_players) \n# get current date\ncurrent_date <- as_datetime(Sys.Date())\n# update data frame with mutated column that adds current_date\ndf <- df %>% \n  mutate(date = current_date)\nNow lets see the first 6 rows of our new data frame that we crafted using rvest.\n\nhead(df)\n\n[1] game            current_players date           \n<0 rows> (or 0-length row.names)"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "I have been practicing with Quarto for a couple of weeks now, and have enjoyed it so much! Quarto has created many more opportunities for myself and the types of deliverable I can create. If you are using R or Python and have not gotten to use Quarto yet, then this is your sign to look into how awesome it is!\nClick the image below to go to Quarto‚Äôs Website"
  }
]